<h1 id="-how-understanding-valence-could-help-make-future-ais-safer-https-opentheory-net-2015-09-fai_and_valence-"><a href="https://opentheory.net/2015/09/fai_and_valence/">How understanding valence could help make future AIs safer</a></h1>
<p>This article is like thunder through my ears, Michael&#39;s arguments speaks to me intuitively, they matched many of my own musings yet better articulated than I could.</p>
<p>I agree with almost all points raised in this article, except the point about wire-heading (I guess I&#39;m more skeptical/confused about the concept of wire-heading itself than Michaels agrument about it).</p>
<p>The first point Michael made about valence and value-loading resonates with me the most. How are we going to know what should AGI do, if we don&#39;t understand what their objects of effect desires, and are there any universal principles of those desire? It&#39;s as if doing optimization theory research without looking into the data distributions empirically. </p>
<p>Working on valence helps AGI safety yet is orthogonal to speeding up AGI is also a very good point.</p>
<p>The article is very thought-provoking to me, the following is the loosely organized thoughts I have while reading it.</p>
<h2 id="my-deep-confusions-about-wire-heading-">My deep confusions about wire-heading:</h2>
<p>The idea that the utility and valence of an agent are different things is such a subtle point and Michael agrued much better than me in my mind. But thinking of it, there&#39;s many unknowns about it (to me at least, it is likely that I&#39;m just deeply ignorant about agents and utilities etc and what&#39;s written below doesn&#39;t make sense).</p>
<p>(I&#39;m about to study Sutton&#39;s RL after finishing the batch of books I&#39;m currently reading, and I will update on any misunderstandings after that).</p>
<ul>
<li><p>External, loaded utility function in the RL sense vs the <em>true</em> objective of an agent in the formal sense are different concepts and should be better distinguished.<br>  If subjective valence affects the agent&#39;s behavior at all, it should be included as part of the formal objective of the agent. The fact that they&#39;re discussed as separate here probably indicates Michael meant to talk about objective in the RL sense, which is loaded externally.<br>  (If that&#39;s not the case than I must have been confused about what utility and valence mean here.)</p>
</li>
<li><p>Thinking about wire-heading reveals that I don&#39;t understand the relationship between subjective valence, utility and behavior.</p>
</li>
<li><p><em>How does valence and behavior interact with each other? What do we mean that we want something (is it because that&#39;s described by utility and our model is trained/guided by it, or because we feel good so we want it)?</em>  </p>
</li>
<li><p><em>Do we (and agents with subjective experience in general) want to move to a state with higher valence fundamentally, or it&#39;s a some how learned policy?</em><br>  I think it&#39;s the former and that might have sth to do with what valence is?<br>  If it&#39;s the latter then it&#39;s really bizarre why that&#39;s picked up by evolution, and it&#39;s also bad news that this might not be universal.</p>
</li>
<li><p><em>Which comes first?</em><br>  <em>either we have objectives as agents and we find states that matches the objective as pleasent via self-modeling;</em><br>  <em>or first exist fundamental principles of valence and objectives evolved to fit criteria of self-replication toward positive valence in agents;</em><br>  <em>or they&#39;re separate things, have more complex relationship with each other?</em><br>  I think valence comes first, and evolution picked up on it (related to last q); but also if valence is low-complexity concept (so many different agent designs represent similar valence level etc), there&#39;s likely exist ways to morph it in the design space to match a existing objective. Not sure whether it make sense or not but it&#39;s very confusing.  </p>
</li>
<li><p>I&#39;m not convinced by the significance of the wire-heading concept<br>  I don&#39;t think I understand the concept deeply (I&#39;ve read Superintelligence and find it confusing back then). But if it literally means what mice did in the OG wire-heading experiment, whatever positive and negative experience we persue can be seen as a less extreme version of wire-heading! Wire-heading (of this definition) isn&#39;t a binary concept. And suddenly most of the elements in the set of wire-heading behavior felt much less deadly! Like sex, sleep, friendship, learning sth new...  </p>
</li>
<li><p>I think treating objectives as some external oracle has its blind-spots (this is what I believe the wire-heading argument stands).<br>  Following the previous point. For us human minds, the part of brain/mind that evaluates the the state we&#39;re in are also part of the brain. The question isn&#39;t whether we can directly manipulate the <em>utility</em> but how dexterous we are when interfacing with it, how different utilities are coherent or not, what does utility reacts to self-referential behavior etc. I&#39;m not sure if the same is true for AI universally, and most AI today aren&#39;t smart enough to show such behavior so I can&#39;t think of any good examples to work with.<br>  <del>Currently human utilities are greatly under-actuated, but if those utility evaluators can work with so many complex states, how likely is it that they are also agents themselves with their own feedback cycle of actions and their subsequent effects?</del> don&#39;t know where this come from<br>  And even though it looks clean and clear to distinguish input of environment state and reward signal in math formalism, I always find it fishy. Seems like what we choose as evaluation for reward (by parts of ourselves) and feed the evaluation to other parts of the agent/mind/whatever is important here.  </p>
</li>
</ul>
<h2 id="my-thoughts-about-valence-and-possible-directions">My thoughts about valence and possible directions</h2>
<p>I haven&#39;t read QRI&#39;s research on valence yet, and I want to write down what I thought about it beforehand. And I will learn and think more about valence and the possible ways to measure it.</p>
<ul>
<li><p>I used to think of happiness (similar to valence mentioned here) as in a state that one doesn&#39;t want to leave in a sense that it generates coherent predictions from all of its parts (so there&#39;s no conflict and no part want&#39;s to leave or fight for control).<br>  To achieve this in a dynamic world of ours, it needs to be robust towards change while be able to adapt to them, so it prefers states that&#39;s not boring/static (too brittle) yet is predictable/compressable (in the format of the specific mind).<br>  And since it&#39;s active, it has to be spatially and temporally constrained to varify its predictions/manipulations of the world (I also suspect static data have feelings so this make sense to me).<br>  I find it loosely analogous to the free-energy principle.<br>  I used to think of it from the utilitarian perspective: that agents like this will perpetually survive. But now I suspect there&#39;s something more fundamental about it if valence comes first.<br>  I&#39;m not sure if this make sense or not, but it&#39;s really intriguing to me that the two directions match in...this world (robust and adaptive states feel good, and good-feeling states help survive).  </p>
</li>
<li><p>And if I were to work on things adjacent to what Michael has written (intersection of valence and AGI alignment), I might try to figure out sensible metrics of the valence of a general and abstract swarm-y thing, and in particular how utility and valence effect parts of an agent.<br>  If valence is about the distribution of predictions given by different parts (sub-agents), I think the questions is how seriously should we view this claim? As a hand-wavy analogy that part of brains want different things, or as rigorous description that agents can be decomposed to smaller agents with their own utility and models the effects of their actions on the environment (mostly neighbor sub-agents)?<br>  I tend to think it&#39;s the latter. And since this is a fairly strong assumption, it&#39;s going to be very useful to figure out what kinds of agents have such internal structures and how to find them, evaluate their goals, see how different levels of hierarchy relate to each other, etc. If we can recursively break down an agent to its self-similar parts for a wide range of different agents I suspect we can do much in divide-and-conquer sense.</p>
</li>
</ul>

